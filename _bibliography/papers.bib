---
---
@inproceedings{3e584d35d27649a48a0bbd18237710e7,
title = "Vocabulary Profiling of Reading Materials for Learning Chinese",
abstract = "Since extensive reading is important for language learning, students should engage in extra-curricular reading as much as possible. To facilitate efficient learning, language teachers need to select reading materials at the appropriate difficulty level, and adapt them if necessary. This paper describes a text analysis and revision tool that assists teachers in preparing reading materials in Chinese. On the basis of a graded vocabulary list, the tool estimates the percentage of words in a text that are known to students at the target school grade. Further, it highlights words that are expected to be new vocabulary, and suggests alternatives that better fit the expected vocabulary proficiency at the target grade. Evaluation results show that the use of Forward Maximal Matching improves word matching with the vocabulary list, and that dynamic editing of word segmentation leads to more accurate assessment of the text difficulty level. {\textcopyright} 2024 IEEE",
keywords = "automatic readability assessment, computer-assisted language learning, language education, natural language processing",
author = "Lim, {Ho Hung} and Lee, {John S. Y.} and Meichun Liu",
year = "2024",
doi = "10.1109/ICIET60671.2024.10542799",
language = "English",
isbn = "9798350371789",
series = "International Conference on Information and Education Technology, ICIET",
publisher = "IEEE",
pages = "53--57",
booktitle = "2024 12th International Conference on Information and Education Technology, ICIET 2024",
address = "United States",
note = "12th International Conference on Information and Education Technology (ICIET 2024) ; Conference date: 18-03-2024 Through 20-03-2024",
url = "https://www.iciet.org/2024.html",
}

@inproceedings{lim-lee-2024-improving,
    title = "Improving Readability Assessment with Ordinal Log-Loss",
    author = "Lim, Ho Hung  and
      Lee, John",
    editor = {Kochmar, Ekaterina  and
      Bexte, Marie  and
      Burstein, Jill  and
      Horbach, Andrea  and
      Laarmann-Quante, Ronja  and
      Tack, Ana{\"i}s  and
      Yaneva, Victoria  and
      Yuan, Zheng},
    booktitle = "Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.bea-1.28/",
    pages = "343--350",
    abstract = "Automatic Readability Assessment (ARA) predicts the level of difficulty of a text, e.g. at Grade 1 to Grade 12. ARA is an ordinal classification task since the predicted levels follow an underlying order, from easy to difficult. However, most neural ARA models ignore the distance between the gold level and predicted level, treating all levels as independent labels. This paper investigates whether distance-sensitive loss functions can improve ARA performance. We evaluate a variety of loss functions on neural ARA models, and show that ordinal log-loss can produce statistically significant improvement over the standard cross-entropy loss in terms of adjacent accuracy in a majority of our datasets."
}

@inproceedings{86e3eba58164429ba454c41c11a03ff7,
title = "Robustness of Hybrid Models in Cross-domain Readability Assessment",
abstract = "Recent studies in automatic readability assessment have shown that hybrid models — models that leverage both linguistically motivated features and neural models — can outperform neural models. However, most evaluations on hybrid models have been based on in-domain data in English. This paper provides further evidence on the contribution of linguistic features by reporting the first direct comparison between hybrid, neural and linguistic models on cross-domain data. In experiments on a Chinese dataset, the hybrid model outperforms the neural model on both in-domain and cross-domain data. Importantly, the hybrid model exhibits much smaller performance degradation in the cross-domain setting, suggesting that the linguistic features are more robust and can better capture salient indicators of text difficulty.",
author = "Lim, {Ho Hung} and Tianyuan Cai and Lee, {John S. Y.} and Meichun Liu",
year = "2022",
month = dec,
language = "English",
series = "Proceedings of the Australasian Language Technology Workshop",
publisher = "Australasian Language Technology Association",
pages = "62--67",
booktitle = "Proceedings of the 20th Workshop of the Australasian Language Technology Association",
note = "20th Annual Workshop of the Australasian Language Technology Association (ALTA 2022) ; Conference date: 14-12-2022 Through 16-12-2022",
url = "https://alta2022.alta.asn.au/",
}

@inproceedings{lee-etal-2022-automatic,
    title = "Automatic Nominalization of Clauses through Textual Entailment",
    author = "Lee, John S. Y.  and
      Lim, Ho Hung  and
      Webster, Carol  and
      Melser, Anton",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.524/",
    pages = "6002--6006",
    abstract = "Nominalization re-writes a clause as a noun phrase. It requires the transformation of the head verb of the clause into a deverbal noun, and the verb{'}s modifiers into nominal modifiers. Past research has focused on the selection of deverbal nouns, but has paid less attention to predicting the word positions and word forms for the nominal modifiers. We propose the use of a textual entailment model for clause nominalization. We obtained the best performance by fine-tuning a textual entailment model on this task, outperforming a number of unsupervised approaches using language model scores from a state-of-the-art neural language model."
}

@inproceedings{7abe1b3ef79742e7b4b8a5738201cb72,
title = "Enhancing Automatic Readability Assessment with Verb Frame Features",
abstract = "Patterns of verb usage, as encoded in verb frame elements, have been shown to correlate with text readability. While automatic readability assessment models have incorporated a wide range of features on lexical, syntactic, semantic and discourse properties of a text, none of these features directly exploit the semantic information provided in verb frames. This paper investigates whether features based on semantic frames can improve the performance of Chinese readability assessment models. We conduct experiments with manually annotated verb frame elements in Mandarin VerbNet, including the use of non-core frame elements, subject omission, metaphoric usage and clause as verb argument. Experimental results on a corpus of Chinese texts, spanning twelve school grades, show that these frame features significantly raise readability assessment accuracy over a baseline model that relies on surface features only.",
keywords = "automatic readability assessment, Chinese, Mandarin VerbNet, verb frames",
author = "Tianyuan CAI and LIM, {Ho Hung} and LEE, {John S. Y.} and Meichun LIU",
year = "2022",
doi = "10.1109/IALP57159.2022.9961289",
language = "English",
isbn = "978-1-6654-7675-1",
series = "International Conference on Asian Language Processing, IALP",
publisher = "IEEE",
pages = "413--418",
editor = "Rong Tong and Yanfeng Lu and Minghui Dong and Wengao Gong and Haizhou Li",
booktitle = "2022 International Conference on Asian Language Processing (IALP)",
address = "United States",
note = "2022 International Conference on Asian Language Processing (IALP 2022) ; Conference date: 27-10-2022 Through 28-10-2022",
url = "https://www.colips.org/conferences/ialp2022/wp/venue/",
}

@inproceedings{lee-etal-2022-unsupervised,
    title = "Unsupervised Paraphrasability Prediction for Compound Nominalizations",
    author = "Lee, John Sie Yuen  and
      Lim, Ho Hung  and
      Webster, Carol",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.237/",
    doi = "10.18653/v1/2022.naacl-main.237",
    pages = "3254--3263",
    abstract = "Commonly found in academic and formal texts, a nominalization uses a deverbal noun to describe an event associated with its corresponding verb. Nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Automatic generation of clausal paraphrases for nominalizations can help disambiguate their meaning. However, previous work has not identified cases where it is awkward or impossible to paraphrase a compound nominalization. This paper investigates unsupervised prediction of paraphrasability, which determines whether the prenominal modifier of a nominalization can be re-written as a noun or adverb in a clausal paraphrase. We adopt the approach of overgenerating candidate paraphrases followed by candidate ranking with a neural language model. In experiments on an English dataset, we show that features from an Abstract Meaning Representation graph lead to statistically significant improvement in both paraphrasability prediction and paraphrase generation."
}

@inproceedings{lee-etal-2021-paraphrasing,
    title = "Paraphrasing Compound Nominalizations",
    author = "Lee, John  and
      Lim, Ho Hung  and
      Webster, Carol",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.632/",
    doi = "10.18653/v1/2021.emnlp-main.632",
    pages = "8023--8028",
    abstract = "A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of unsupervised methods, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model."
}

